# Reflection

I am most confident in the entire data flow and how the architecture merges numerous cloud services into a unified pipeline.  The usage of Azure Blob Storage as a landing zone, Azure Functions for event-driven ETL, and Azure SQL Database for structured reporting is consistent with the course modules and shows a clear division of duties.  I am also confident in the security and governance strategy, specifically the usage of Key Vault and managed identities for credential management, as well as the use of RBAC with Entra ID to enforce least privilege.  These features are simple, well-documented, and correspond closely to the best practices we examined.

I am less clear about the analytics and orchestration layers. A scheduled Azure ML Notebook is suitable for small-scale summaries, but in a production setting, a more robust orchestration tool, such as Azure Data Factory or Synapse pipelines, may be required to manage larger datasets, retries, and monitoring. I am particularly concerned about the variety of device APIs and the difficulty of standardizing data from multiple sources. This could present issues in schema design and validation that extend beyond the bounds of the prototype.

One alternative architecture I examined was building the solution on Google Cloud Platform.  In that scenario, Cloud Run would host the Flask app, Cloud Functions would handle ETL, Cloud Storage would act as the landing zone, and BigQuery would handle analytics and reporting.  While this technique simplifies large-scale analytics, I elected not to pursue it because the course focused on Azure services and managed identity integration, and I wanted to stick with the tools we used.  Another option was to containerize all components with Azure Container Apps, although this increased operational overhead when contrasted to the simplicity of serverless operations.

If I had four to eight more weeks and limitless credits, I would expand the solution in a variety of ways.  First, I would set up a de-identification pipeline using reversible tokenization and stringent key management to get the system ready for real PHI.  Second, I would include a BI layer, such as Power BI, to give physicians with more detailed visualizations and drill-down capabilities.  Third, I would add a rules engine that can be configured to support patient-specific criteria and automated notifications sent via email or SMS.  Finally, I would prioritize interoperability by mapping summarized data to FHIR resources, allowing for downstream integration with electronic health record systems.  The following stages would develop the prototype into a production-ready, compliant healthcare cloud service.

