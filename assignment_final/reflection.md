# Reflection

I feel most confident about the overall data flow and the way the architecture integrates multiple cloud services into a coherent pipeline. The use of Azure Blob Storage as a landing zone, Azure Functions for event-driven ETL, and Azure SQL Database for structured reporting aligns well with the course modules and demonstrates clear separation of responsibilities. I am also confident in the security and governance approach, particularly the use of Key Vault and managed identities to handle credentials, and the application of RBAC through Entra ID to enforce least privilege. These elements are straightforward, well-documented, and map directly to best practices we studied.

Where I am less certain is in the analytics and orchestration layer. While a scheduled Azure ML Notebook is sufficient for small-scale summaries, in a production environment a more robust orchestration tool such as Azure Data Factory or Synapse pipelines might be necessary to handle larger datasets, retries, and monitoring. I am also unsure about the variability of device APIs and the complexity of normalizing data across different sources. This could introduce challenges in schema design and validation that go beyond the prototype scope.

An alternative architecture I considered was building the solution on Google Cloud Platform. In that design, Cloud Run would host the Flask app, Cloud Functions would handle ETL, Cloud Storage would serve as the landing zone, and BigQuery would provide analytics and reporting. While this approach simplifies large-scale analytics, I chose not to pursue it because the course emphasized Azure services and managed identity integration, and I wanted to stay consistent with the tools we practiced. Another alternative was to containerize all components using Azure Container Apps, but this added operational overhead compared to the simplicity of serverless functions.

If I had four to eight more weeks and unlimited credits, I would expand the solution in several ways. First, I would implement a de-identification pipeline with reversible tokenization and strict key management to prepare the system for handling real PHI. Second, I would integrate a BI layer such as Power BI to provide richer visualizations and drill-down capabilities for clinicians. Third, I would add a configurable rules engine to support patient-specific thresholds and automated alerts routed via email or SMS. Finally, I would focus on interoperability by mapping the summarized data into FHIR resources, enabling downstream integration with electronic health record systems. These next steps would transform the prototype into a production-ready, compliant healthcare cloud solution.

